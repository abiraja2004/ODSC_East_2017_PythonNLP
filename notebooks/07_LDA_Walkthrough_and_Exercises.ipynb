{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "<tr>\n",
    "\n",
    "<th, style=\"background-color:white\">\n",
    "<img src=\"https://github.com/mlgill/ODSC_East_2017_PythonNLP/blob/master/assets/logo.png?raw=true\", width=140, height=100>\n",
    "</th>\n",
    "\n",
    "<th, style=\"background-color:white\">\n",
    "<div align=\"left\">\n",
    "<h1>Learning from Text: <br> Introduction to Natural Language Processing with Python</h1>  \n",
    "<h2>Michelle L. Gill, Ph.D.</h2>     \n",
    "Senior Data Scientist, Metis  \n",
    "ODSC East  \n",
    "May 3, 2017 \n",
    "</div>\n",
    "</th>\n",
    "\n",
    "</tr>\n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Walkthrough and Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## The Data\n",
    "\n",
    "We will be using a portion data set containing approximately 20,000 posts partitioned evenly across 20 different newsgroups. This data set is quite famous. We will be using a sample of this data set, containing 5 topics and about 3,000 posts.\n",
    "\n",
    "We will begin by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T10:21:31.684450Z",
     "start_time": "2017-05-03T10:21:30.192639Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from accessory_functions import nltk_path\n",
    "\n",
    "# Setup nltk corpora path\n",
    "nltk.data.path.insert(0, nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-03T10:21:30.197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2956\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "topic_list = ['sci.space', 'comp.sys.mac.hardware', 'rec.autos',\n",
    "              'rec.sport.baseball', 'sci.med']\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, data_home='../data',\n",
    "                             categories=topic_list,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data = pd.DataFrame(dataset['data'], columns=['text'])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "\n",
    "Next we will preprocess the data using the convenience method from `accessory_functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-03T10:21:30.200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from accessory_functions import preprocess_series_text\n",
    "\n",
    "data['text'] = preprocess_series_text(data.text, \n",
    "                                      nltk_path=nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-03T10:21:30.203Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Numerical Features\n",
    "\n",
    "Use Count Vectorizer to create a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-03T10:21:30.206Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_features = 1000\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, \n",
    "                     max_features=n_features)\n",
    "X = cv.fit_transform(data.text)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-03T10:21:30.213Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an LDA Model\n",
    "\n",
    "Use Scikit-learn's [`LatentDirichletAllocation`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) to fit an LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-03T10:21:30.216Z"
    }
   },
   "outputs": [],
   "source": [
    "LatentDirichletAllocation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-03T10:21:30.219Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = len(topic_list)\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-01T22:43:33.917614Z",
     "start_time": "2017-05-01T22:43:33.914521Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Print Top Words\n",
    "Print the top words associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-03T10:21:30.221Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % (topic_idx+1))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-03T10:21:30.223Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "n_top_words = 20\n",
    "\n",
    "print(\"Topics in LDA model:\")\n",
    "cv_feature_names = cv.get_feature_names()\n",
    "print_top_words(lda, cv_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-01T22:45:20.581963Z",
     "start_time": "2017-05-01T22:45:20.560132Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Visualize the LDA Model\n",
    "\n",
    "A visualization of the topic model can be easily created with `pyLDAvis`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-03T10:21:30.230Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda, X, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "* Fit an LDA model with a different number of topics and compare the top 20 words to those from the model above.\n",
    "* Create a different document-term matrix by changing input parameters (max_features, etc.) or by switching to `TfidfVectorizer` and use this to fit another LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
